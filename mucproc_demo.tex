\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{hyperref}

\title{N-gram Language Models}
\author{
  Felix \\
  \and
  Borisov Timofei \\
  \and
  Simon \\
}
\date{\today}

\begin{document}

% Title page
\maketitle

% Abstract
\begin{abstract}
This is a brief summary of your essay. It should be concise and informative.
\end{abstract}

% Table of contents
\tableofcontents
\newpage

% Sections
\section{Introduction}
This is the introduction section where you provide background information on your topic and outline the structure of your essay.

\section{Main Section 1 (FELIX)}
\subsection{Subsection 1.1}
Here you can start discussing the details of your first main point. For example, algorithms are a fundamental part of computer science and understanding them is crucial for any software developer. As stated in \cite{cormen2009}, algorithms are essential for efficient problem solving in computing.

\subsection{Subsection 1.2}
Continue with further details and analysis related to your first main point.

\section{Fortgeschrittene Konzepte und Techniken in N-Gramm-Modellen (Borisov Timofei)}
\subsection{Was ist un-seen N-Grams?}
Da jeder Corpus begrenzt ist, fehlen darin zwangsläufig einige völlig akzeptable Wortfolgen. Das heißt, wir werden viele Fälle von vermeintlichen "zero probability n-grams" haben, die in Wirklichkeit eine Nicht-Null-Wahrscheinlichkeit haben sollten \cite{jurafsky2023}. 

Betrachten wir die Wörter, die auf das Bigram aus Daniel Jurafskys Buch basieren. Ein Textkorpus zusammen mit ihrer Anzahl:

\begin{itemize}
  \item denied the allegations: 5
  \item denied the speculation: 2
  \item denied the rumors: 1
  \item denied the report: 1
\end{itemize}

Aber nehmen wir an, unser Testsatz enthält Sätze wie:
\begin{itemize}
  \item denied the offer
  \item denied the loan
\end{itemize}

P(offer|denied the) ist 0!
P(loan|denied the) ist 0!

Diese Nullen bedeuten, dass wir die Wahrscheinlichkeit anderer Wortkombinationen stark unterschätzt haben, was die richtige Entscheidung der Anwendung, die auf unserem Modell läuft, stark beeinflussen könnte.

Das Problem heißt “Data Sparsity” \cite{dremio2023}. Also kurz gesagt, das bedeutet, dass viele Daten in einem Datensatz fehlen oder auf Null gesetzt sind, sodass die meisten Zellen in einer Tabelle leer bleiben. Dies tritt häufig bei spärlichen Matrizen oder hochdimensionalen Datensätzen auf, wo nicht alle Elemente beobachtet oder erfasst werden. Was können wir also mit den Wörtern machen, die wir verwenden und die nicht in den Kontext unserer Trainingsdaten passen?

Die Methode zur Lösung dieses Problems heißt: Smoothing oder Discounting.

\subsection{Smoothing Techniques}
Further details and analysis related to your second main point.

\subsection{Vergleich von N-Grammen und neuronalen Netzen}
Further details and analysis related to your second main point.

\section{Main Section 3 (SIMON)}
\subsection{Subsection 3.1}
Discuss the details of your third main point.
\subsection{Subsection 3.2}
Further details and analysis related to your third main point.

\section{Conclusion}
Summarize the main points discussed in your essay and provide your final thoughts.

% Bibliography
\newpage
\begin{thebibliography}{9}

\bibitem{cormen2009}
Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein, \textit{Introduction to Algorithms}, 3rd Edition, MIT Press, 2009.

\bibitem{jurafsky2023}
Daniel Jurafsky and James H. Martin, \textit{Speech and Language Processing}, 3rd Edition, Pearson, 2023.

\bibitem{dremio2023}
Dremio, ``Data Sparsity,'' \url{https://www.dremio.com/wiki/data-sparsity/}, Accessed: June 1, 2024.

\bibitem{example2}
Author Name, ``Article Title,'' \textit{Journal Name}, Volume(Issue), pages, Year.

\bibitem{example3}
Author Name, ``Webpage Title,'' \url{http://example.com}, Accessed: Date.

\end{thebibliography}

\end{document}
